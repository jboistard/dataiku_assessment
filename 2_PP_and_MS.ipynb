{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Dataiku Data Scientist Technical Assessment**\n",
    "### Author : Jules Boistard\n",
    "### Submission date : January 18th, 2022\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **#2 : Pre-processing and preliminary model selection**\n",
    "\n",
    "The goal here is to iteratively find the best pre-processing pipeline based on a simple basic machine learning model, adding/removing steps along the way.\n",
    "\n",
    "This section **does not** aim at finding the best performing model. This will be the focus of the next section (\"model selection\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **#2.1 : Importing and splitting data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the train and test files without duplicates and conflicts\n",
    "# NB : we do not want instance weight for machine learning training\n",
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv(\"data/train_clean.csv\", header=0)\n",
    "test = pd.read_csv(\"data/test_clean.csv\", header=0)\n",
    "data = pd.concat([train, test])\n",
    "\n",
    "# Split datasets into features set and target variable\n",
    "# Note : target classes \"-50k\" and \">50k\" will respectively be labelled 0 and 1\n",
    "X_train_raw = train.iloc[:, :-2]\n",
    "X_test_raw = test.iloc[:, :-2]\n",
    "y_train = train.iloc[:, -2].map(lambda s: 0 if \"-\" in s else 1)\n",
    "y_test = test.iloc[:, -2].map(lambda s: 0 if \"-\" in s else 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **#2.2 : Pre processing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first pre-processing pipeline will consist in 4 basic steps :\n",
    "1. Drop columns with ~50% NaN Values (see part 1 : EDA)\n",
    "2. Impute NaN values with most frequent class for other columns wit missing values (see part 1 : EDA)\n",
    "3. Encode our remaining categorical features with one hot encoding\n",
    "4. Perform standard scaling for our numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us first define different sets of features\n",
    "cols_to_drop = [\"migration code-change in msa\", \"migration code-change in reg\", \"migration code-move within reg\", \"migration prev res in sunbelt\"]\n",
    "num_cols = [\"age\", \"wage per hour\", \"capital gains\", \"capital losses\", \"dividends from stocks\", \"weeks worked in year\"] # \"num persons worked for employer\" is removed because of mapping\n",
    "cat_cols = list(set(X_train_raw.columns).difference(set(num_cols)).difference(set(cols_to_drop)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop useless columns\n",
    "X_train = X_train_raw.drop(cols_to_drop, axis=1)\n",
    "X_test = X_test_raw.drop(cols_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputer for missing value with most frequent category (numerical features don't have missing values)\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"most_frequent\").fit(pd.concat([X_train, X_test]))\n",
    "\n",
    "X_train = pd.DataFrame(imputer.transform(X_train), columns=X_train.columns)\n",
    "X_test = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding for categorical variables\n",
    "# Note : we will drop the 0 / Not in universe category for each variable if it exists, othewise the first one by default, to avoid multicollinearity\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "drop_cat = []\n",
    "for col in cat_cols:\n",
    "    cat_values = list(X_train[col].unique())    \n",
    "    if 0 in cat_values:\n",
    "        drop_cat.append(0)\n",
    "    elif \"Not in universe\" in cat_values:\n",
    "        drop_cat.append(\"Not in universe\")\n",
    "    else:\n",
    "        drop_cat.append(cat_values[0])\n",
    "\n",
    "encoder = OneHotEncoder(drop=drop_cat, sparse=False, dtype=\"int32\", handle_unknown=\"ignore\").fit(pd.concat([X_train[cat_cols], X_test[cat_cols]]))\n",
    "new_cat_cols = encoder.get_feature_names_out()\n",
    "\n",
    "X_train_onehot = pd.DataFrame(encoder.transform(X_train[cat_cols]), columns=new_cat_cols)\n",
    "X_test_onehot = pd.DataFrame(encoder.transform(X_test[cat_cols]), columns=new_cat_cols)\n",
    "\n",
    "X_train = pd.concat([X_train_onehot, X_train[num_cols]], axis=1)\n",
    "X_test = pd.concat([X_test_onehot, X_test[num_cols]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale numerical columns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler().fit(pd.concat([X_train[num_cols], X_test[num_cols]]))\n",
    "col_names = scaler.get_feature_names_out()\n",
    "\n",
    "X_train_scaled = pd.DataFrame(scaler.transform(X_train[num_cols]), columns=col_names)\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test[num_cols]), columns=col_names)\n",
    "\n",
    "X_train = pd.concat([X_train_scaled, X_train[new_cat_cols]], axis=1)\n",
    "X_test = pd.concat([X_test_scaled, X_test[new_cat_cols]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **#2.3 : Baseline models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin with, we will compare two very basic models, on which we will perform minimal pre-processing :\n",
    "- One linear : a logistic regression model\n",
    "- One non linear : a decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation procedure\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import learning_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_model(model, X_train, y_train, X_test, y_test, curves=True):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    print(classification_report(y_test, y_pred, target_names=[\"Low income (<50k)\", \"High income (>50k)\"], digits=4))\n",
    "\n",
    "    if curves:\n",
    "        N, train_score, val_score = learning_curve(model, X_train, y_train, scoring=\"f1_macro\")\n",
    "        plt.figure()\n",
    "        plt.plot(N, train_score.mean(axis=1), label=\"train score\")\n",
    "        plt.plot(N, val_score.mean(axis=1), label=\"val score\")\n",
    "        plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important** : we do not value one of the 2 target classes more than the other. In other words, good predictions are equally important for the \"low income\" and \"high income\" classes. On the other hand, accuracy will not do because of class imbalance (~94%/6%). We can therefore choose the **macro averaged f1-score** as our performance metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      " Low income (<50k)     0.9870    0.8435    0.9096     72669\n",
      "High income (>50k)     0.3199    0.8689    0.4677      6157\n",
      "\n",
      "          accuracy                         0.8455     78826\n",
      "         macro avg     0.6535    0.8562    0.6886     78826\n",
      "      weighted avg     0.9349    0.8455    0.8751     78826\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Building and evaluating a baseline logistic regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_clf = LogisticRegression(max_iter=5000, class_weight=\"balanced\", random_state=42)\n",
    "evaluate_model(lr_clf, X_train, y_train, X_test, y_test, curves=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      " Low income (<50k)     0.9576    0.9524    0.9549     72669\n",
      "High income (>50k)     0.4715    0.5017    0.4862      6157\n",
      "\n",
      "          accuracy                         0.9172     78826\n",
      "         macro avg     0.7145    0.7270    0.7205     78826\n",
      "      weighted avg     0.9196    0.9172    0.9183     78826\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Building and evaluating a baseline decision tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt_clf = DecisionTreeClassifier(criterion=\"entropy\", class_weight=\"balanced\", random_state=42)\n",
    "evaluate_model(dt_clf, X_train, y_train, X_test, y_test, curves=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple things worth noting :\n",
    "- The **decision tree performs better** than the logistic regression model. We will however continue to compare the both of them throughout the next steps.\n",
    "- Depending on the context, we could favor one based on individual f1, recall or sensitivity scores for one of the two classes.\n",
    "- Overall, there is room for improvement. While the f1 score is not too bad on the majority class (low income) it is, as could be expected, quite poor for the minority class (high income).\n",
    "\n",
    "To adress the issue of underfitting, we will try the following pre-processing operations :\n",
    "- Perform over-sampling to increase the representation of the minority class\n",
    "- Perform some feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before going any further, let us take a look at feature conctribution in both models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_contrib = pd.concat([\n",
    "    pd.DataFrame(data=lr_clf.coef_.flatten(), index=X_train.columns, columns=[\"log reg coef\"]),\n",
    "    pd.DataFrame(data=dt_clf.feature_importances_, index=X_train.columns, columns=[\"tree contrib\"])\n",
    "], axis=1)\n",
    "feature_contrib[\"log reg coef abs\"] = abs(feature_contrib[\"log reg coef\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Top contributing features for log reg =====\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>log reg coef</th>\n",
       "      <th>tree contrib</th>\n",
       "      <th>log reg coef abs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>education_Doctorate degree(PhD EdD)</th>\n",
       "      <td>2.307</td>\n",
       "      <td>0.004</td>\n",
       "      <td>2.307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>country of birth self_Nicaragua</th>\n",
       "      <td>-2.038</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>country of birth father_Guatemala</th>\n",
       "      <td>-2.009</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>education_Prof school degree (MD DDS DVM LLB JD)</th>\n",
       "      <td>1.895</td>\n",
       "      <td>0.003</td>\n",
       "      <td>1.895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tax filer stat_Joint both under 65</th>\n",
       "      <td>1.891</td>\n",
       "      <td>0.003</td>\n",
       "      <td>1.891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>country of birth self_Scotland</th>\n",
       "      <td>1.767</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>country of birth self_Guatemala</th>\n",
       "      <td>1.764</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>education_Masters degree(MA MS MEng MEd MSW MBA)</th>\n",
       "      <td>1.673</td>\n",
       "      <td>0.009</td>\n",
       "      <td>1.673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tax filer stat_Head of household</th>\n",
       "      <td>1.499</td>\n",
       "      <td>0.002</td>\n",
       "      <td>1.499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>education_Less than 1st grade</th>\n",
       "      <td>-1.479</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.479</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  log reg coef  tree contrib  \\\n",
       "education_Doctorate degree(PhD EdD)                      2.307         0.004   \n",
       "country of birth self_Nicaragua                         -2.038         0.000   \n",
       "country of birth father_Guatemala                       -2.009         0.000   \n",
       "education_Prof school degree (MD DDS DVM LLB JD)         1.895         0.003   \n",
       "tax filer stat_Joint both under 65                       1.891         0.003   \n",
       "country of birth self_Scotland                           1.767         0.000   \n",
       "country of birth self_Guatemala                          1.764         0.000   \n",
       "education_Masters degree(MA MS MEng MEd MSW MBA)         1.673         0.009   \n",
       "tax filer stat_Head of household                         1.499         0.002   \n",
       "education_Less than 1st grade                           -1.479         0.000   \n",
       "\n",
       "                                                  log reg coef abs  \n",
       "education_Doctorate degree(PhD EdD)                          2.307  \n",
       "country of birth self_Nicaragua                              2.038  \n",
       "country of birth father_Guatemala                            2.009  \n",
       "education_Prof school degree (MD DDS DVM LLB JD)             1.895  \n",
       "tax filer stat_Joint both under 65                           1.891  \n",
       "country of birth self_Scotland                               1.767  \n",
       "country of birth self_Guatemala                              1.764  \n",
       "education_Masters degree(MA MS MEng MEd MSW MBA)             1.673  \n",
       "tax filer stat_Head of household                             1.499  \n",
       "education_Less than 1st grade                                1.479  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "print(\"===== Top contributing features for log reg =====\")\n",
    "feature_contrib.sort_values(\"log reg coef abs\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Top contributing features for decision tree =====\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>log reg coef</th>\n",
       "      <th>tree contrib</th>\n",
       "      <th>log reg coef abs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>weeks worked in year</th>\n",
       "      <td>0.913</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>0.632</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dividends from stocks</th>\n",
       "      <td>0.497</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sex_Female</th>\n",
       "      <td>-1.106</td>\n",
       "      <td>0.047</td>\n",
       "      <td>1.106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>capital gains</th>\n",
       "      <td>0.837</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>major occupation code_Professional specialty</th>\n",
       "      <td>0.810</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>major occupation code_Executive admin and managerial</th>\n",
       "      <td>0.480</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>capital losses</th>\n",
       "      <td>0.198</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>education_Bachelors degree(BA AB BS)</th>\n",
       "      <td>1.062</td>\n",
       "      <td>0.012</td>\n",
       "      <td>1.062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num persons worked for employer_6</th>\n",
       "      <td>0.521</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.521</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    log reg coef  \\\n",
       "weeks worked in year                                       0.913   \n",
       "age                                                        0.632   \n",
       "dividends from stocks                                      0.497   \n",
       "sex_Female                                                -1.106   \n",
       "capital gains                                              0.837   \n",
       "major occupation code_Professional specialty               0.810   \n",
       "major occupation code_Executive admin and manag...         0.480   \n",
       "capital losses                                             0.198   \n",
       "education_Bachelors degree(BA AB BS)                       1.062   \n",
       "num persons worked for employer_6                          0.521   \n",
       "\n",
       "                                                    tree contrib  \\\n",
       "weeks worked in year                                       0.187   \n",
       "age                                                        0.136   \n",
       "dividends from stocks                                      0.110   \n",
       "sex_Female                                                 0.047   \n",
       "capital gains                                              0.045   \n",
       "major occupation code_Professional specialty               0.023   \n",
       "major occupation code_Executive admin and manag...         0.019   \n",
       "capital losses                                             0.016   \n",
       "education_Bachelors degree(BA AB BS)                       0.012   \n",
       "num persons worked for employer_6                          0.011   \n",
       "\n",
       "                                                    log reg coef abs  \n",
       "weeks worked in year                                           0.913  \n",
       "age                                                            0.632  \n",
       "dividends from stocks                                          0.497  \n",
       "sex_Female                                                     1.106  \n",
       "capital gains                                                  0.837  \n",
       "major occupation code_Professional specialty                   0.810  \n",
       "major occupation code_Executive admin and manag...             0.480  \n",
       "capital losses                                                 0.198  \n",
       "education_Bachelors degree(BA AB BS)                           1.062  \n",
       "num persons worked for employer_6                              0.521  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"===== Top contributing features for decision tree =====\")\n",
    "feature_contrib.sort_values(\"tree contrib\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can spot some interesting things that corroborate some of the insights from EDA (see part 1) :\n",
    "- 3 of the highest academic degrees were decisive for the \"high income\" class in the log reg model (positive coef)\n",
    "- In decision trees, the feature that gives the maximum information gain is the number of weeks worked during the year.\n",
    "- The \"women feature\" skews the log reg model towards the \"low income\" class (negative coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **#2.3 : Over sampling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try and perform over sampling through a SMOTE algorithm (Synthetic Minority Oversampling TEchnique). The goal is to artificially create observations for the minority class \"in between\" real observations from the same class in the feature space, up until the two classes have an equal number of observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try and perform over sampling to balance classes out\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "X_train_resampled, y_train_resampled = SMOTE(sampling_strategy=\"auto\", random_state=42).fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      " Low income (<50k)     0.9643    0.9447    0.9544     72669\n",
      "High income (>50k)     0.4736    0.5873    0.5244      6157\n",
      "\n",
      "          accuracy                         0.9168     78826\n",
      "         macro avg     0.7190    0.7660    0.7394     78826\n",
      "      weighted avg     0.9260    0.9168    0.9208     78826\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate with log reg model\n",
    "evaluate_model(lr_clf, X_train_resampled, y_train_resampled, X_test, y_test, curves=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      " Low income (<50k)     0.9592    0.9384    0.9487     72669\n",
      "High income (>50k)     0.4209    0.5283    0.4685      6157\n",
      "\n",
      "          accuracy                         0.9064     78826\n",
      "         macro avg     0.6900    0.7334    0.7086     78826\n",
      "      weighted avg     0.9171    0.9064    0.9112     78826\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Same with decision tree\n",
    "evaluate_model(dt_clf, X_train_resampled, y_train_resampled, X_test, y_test, curves=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe two different results :\n",
    "- Oversampling greatly improved the average f1 score of our logistic regression model (we gained 5%)\n",
    "- The same operation dit **not** improve the performance of our decision tree (we lost a little less than 1%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **#2.4 : Feature engineering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have tried adding observations. Let us now try to add features.\n",
    "\n",
    "Since we want to preserve some degree of \"explainability\" in our model, we will not make us of feature engineering algorithm such as polynomial features.\n",
    "\n",
    "However, one interesting step would be to compute our own features. This usually requires domain expertise and, often, gathering extra data. To work with what we have, we can try for example :\n",
    "- To compute an estimate of total earnings based on wage per hour, weeks worked in year and additional incomes such as capital gains and dividends from stocks\n",
    "- To replace \"num persons worked for employer\" category numbers by the segment mean, according to documentation\n",
    "- To compute an estimate of tax rate based on our earnings estimate and tax filling status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average working week is 34.6 hours in the US according to Buereau of Labor Statistics\n",
    "def estimate_earnings(hourly_wage, worked_weeks, capital_gains, capital_losses, dividends, average_hours_in_working_week_us=34.6):\n",
    "    return (\n",
    "        (hourly_wage * worked_weeks * average_hours_in_working_week_us) + capital_gains + dividends - capital_losses\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean segment company sizes, from doc\n",
    "segment_mean_company_size = {\n",
    "    0: 0,\n",
    "    1: (0 + 10)/2,\n",
    "    2: (10 + 24)/2,\n",
    "    3: (25 + 99)/2,\n",
    "    4: (100 + 499)/2,\n",
    "    5: (500 + 999)/2,\n",
    "    6: 2000, # Arbitrary\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''  Tax rates from year 1995 found here : https://taxfoundation.org/historical-income-tax-rates-brackets/\n",
    "\n",
    "Married Filing Jointly \tMarried Filing Separately \tSingle Filer \tHead of Household\n",
    "\n",
    "15.0% \t> \t$0 \t        15.0% \t> \t$0 \t        15.0% \t> \t$0 \t        15.0% \t> \t$0\n",
    "28.0% \t> \t$39,000 \t28.0% \t> \t$19,500 \t28.0% \t> \t$23,350 \t28.0% \t> \t$31,250 \t \n",
    "31.0% \t> \t$94,250 \t31.0% \t> \t$47,125 \t31.0% \t> \t$56,550 \t31.0% \t> \t$80,750 \t \n",
    "36.0% \t> \t$143,600 \t36.0% \t> \t$71,800 \t36.0% \t> \t$117,950 \t36.0% \t> \t$130,800 \t \n",
    "39.6% \t> \t$256,500 \t39.6% \t> \t$128,250 \t39.6% \t> \t$256,500 \t39.6% \t> \t$256,500 \n",
    "'''\n",
    "import numpy as np\n",
    "\n",
    "def estimate_tax_amount(tax_fill_stat, marital_stat, earnings):\n",
    "\n",
    "    tax_rates = [0.15, 0.28, 0.31, 0.36, 0.396]\n",
    "    \n",
    "    if tax_fill_stat == \"Nonfiler\":\n",
    "        return 0\n",
    "    \n",
    "    elif tax_fill_stat == \"Single\":\n",
    "        if \"Married\" in marital_stat: # Married filling separately\n",
    "            earning_brackets = [0, 19500, 47125, 71800, 128250, np.inf]\n",
    "\n",
    "        else : # Single filling\n",
    "            earning_brackets = [0, 23350, 56550, 117950, 256500, np.inf]\n",
    "\n",
    "    elif tax_fill_stat == \"Head of household\":\n",
    "        earning_brackets = [0, 31250, 80750, 130800, 256500, np.inf]\n",
    "\n",
    "    else: # Joint filling\n",
    "        earning_brackets = [0, 39000, 94250, 143600, 256500, np.inf]\n",
    "\n",
    "    amounts_in_brackets= np.clip(earnings, earning_brackets[:-1], earning_brackets[1:]) - earning_brackets[:-1]\n",
    "    tax_amount_estimate = (np.array(tax_rates) * amounts_in_brackets).sum()\n",
    "\n",
    "    return tax_amount_estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_features(X):\n",
    "\n",
    "    X_ = X.copy()\n",
    "    \n",
    "    X_[\"earnings estimate\"] = X_.apply(lambda row: estimate_earnings(\n",
    "        hourly_wage=row[\"wage per hour\"]/100, # Note : according to doc, wage per hour has a 2 decimal implied\n",
    "        worked_weeks=row[\"weeks worked in year\"],\n",
    "        capital_gains=row[\"capital gains\"],\n",
    "        capital_losses=row[\"capital losses\"],\n",
    "        dividends=row[\"dividends from stocks\"]\n",
    "    ), axis=1)\n",
    "\n",
    "    X_[\"nb of employees in company estimate\"] = X_[\"num persons worked for employer\"].map(segment_mean_company_size)\n",
    "\n",
    "    X_[\"tax amount estimate\"] = X_.apply(lambda row: estimate_tax_amount(\n",
    "        tax_fill_stat=row[\"tax filer stat\"],\n",
    "        marital_stat=row[\"marital stat\"],\n",
    "        earnings=row[\"earnings estimate\"]\n",
    "    ), axis=1) \n",
    "\n",
    "    return X_[[\"earnings estimate\", \"nb of employees in company estimate\", \"tax amount estimate\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add features in datasets and scale them\n",
    "X_train_new_features = new_features(X_train_raw)\n",
    "X_test_new_features = new_features(X_test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(pd.concat([X_train_new_features, X_test_new_features]))\n",
    "col_names = scaler.get_feature_names_out()\n",
    "\n",
    "X_train_new_features = pd.concat([X_train, pd.DataFrame(scaler.transform(X_train_new_features), columns=col_names)], axis=1)\n",
    "X_test_new_features = pd.concat([X_test, pd.DataFrame(scaler.transform(X_test_new_features), columns=col_names)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      " Low income (<50k)     0.9870    0.8434    0.9096     72669\n",
      "High income (>50k)     0.3199    0.8691    0.4676      6157\n",
      "\n",
      "          accuracy                         0.8454     78826\n",
      "         macro avg     0.6534    0.8563    0.6886     78826\n",
      "      weighted avg     0.9349    0.8454    0.8751     78826\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Try our new features with the log reg model\n",
    "evaluate_model(lr_clf, X_train_new_features, y_train, X_test_new_features, y_test, curves=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      " Low income (<50k)     0.9578    0.9528    0.9553     72669\n",
      "High income (>50k)     0.4748    0.5041    0.4890      6157\n",
      "\n",
      "          accuracy                         0.9177     78826\n",
      "         macro avg     0.7163    0.7285    0.7222     78826\n",
      "      weighted avg     0.9200    0.9177    0.9188     78826\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Same with decision tree\n",
    "evaluate_model(dt_clf, X_train_new_features, y_train, X_test_new_features, y_test, curves=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This couple of extra features did not quite do the trick. They marginally improved f1 score on the decision tree, but not on the log reg model.\n",
    "\n",
    "Note that this is a very important part of data modelling. As such, feature engineering would probably require a fair bit of extra investigation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before diving into the next part, let us check whether combining SMOTE with our extra features further improves log reg model performance or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      " Low income (<50k)     0.9640    0.9452    0.9545     72669\n",
      "High income (>50k)     0.4741    0.5832    0.5231      6157\n",
      "\n",
      "          accuracy                         0.9169     78826\n",
      "         macro avg     0.7191    0.7642    0.7388     78826\n",
      "      weighted avg     0.9257    0.9169    0.9208     78826\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train_resampled_new_features, y_train_resampled_new_features = SMOTE(sampling_strategy=\"auto\", random_state=42).fit_resample(X_train_new_features, y_train)\n",
    "evaluate_model(lr_clf, X_train_resampled_new_features, y_train_resampled_new_features, X_test_new_features, y_test, curves=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not the case, therefore we will completely abandon extra features for non-tree based models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **#2.5 : More complex models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we found a couple of preprocessing tricks to improve model performance, there is still room for improvement. Before diving into fine-tuning, we can try out a couple of different models, a little bit more elaborate than our two baselines models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      " Low income (<50k)     0.9518    0.9872    0.9692     72669\n",
      "High income (>50k)     0.7308    0.4106    0.5258      6157\n",
      "\n",
      "          accuracy                         0.9422     78826\n",
      "         macro avg     0.8413    0.6989    0.7475     78826\n",
      "      weighted avg     0.9346    0.9422    0.9346     78826\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random forest classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_clf = RandomForestClassifier(class_weight=\"balanced\", random_state=42)\n",
    "\n",
    "evaluate_model(rf_clf, X_train_new_features, y_train, X_test_new_features, y_test, curves=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      " Low income (<50k)     0.9516    0.9894    0.9701     72669\n",
      "High income (>50k)     0.7636    0.4056    0.5298      6157\n",
      "\n",
      "          accuracy                         0.9438     78826\n",
      "         macro avg     0.8576    0.6975    0.7499     78826\n",
      "      weighted avg     0.9369    0.9438    0.9357     78826\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Gradient boosted trees\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb_clf = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "evaluate_model(gb_clf, X_train_new_features, y_train, X_test_new_features, y_test, curves=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      " Low income (<50k)     0.9662    0.9401    0.9530     72669\n",
      "High income (>50k)     0.4640    0.6120    0.5278      6157\n",
      "\n",
      "          accuracy                         0.9145     78826\n",
      "         macro avg     0.7151    0.7760    0.7404     78826\n",
      "      weighted avg     0.9270    0.9145    0.9198     78826\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Stochastic Gradient Descent\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd_clf = SGDClassifier(loss=\"log\", class_weight=\"balanced\", random_state=42)\n",
    "\n",
    "evaluate_model(sgd_clf, X_train_resampled, y_train_resampled, X_test, y_test, curves=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without any fine-tuning, we can already see that all of these models already outperform our two baselines models.\n",
    "\n",
    "They are actually pretty close to each other in terms of performance.\n",
    "\n",
    "In the next section, we will compare them with different sets of hyperparameters to find the best predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **#2.6 : A note on feature selection**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we dive into fine tuning, it is worth noting that feature selection can sometimes improve model performance, getting rid of noise and avoiding dimensionality issues.\n",
    "\n",
    "Since we do not have a major problem of overfitting, we will not perform feature selection here.\n",
    "\n",
    "There is an example of feature selection for the SGD classifier below, yielding a lower f1 score than the \"raw\" equivalent classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      " Low income (<50k)     0.9645    0.9263    0.9450     72669\n",
      "High income (>50k)     0.4070    0.5970    0.4840      6157\n",
      "\n",
      "          accuracy                         0.9006     78826\n",
      "         macro avg     0.6857    0.7617    0.7145     78826\n",
      "      weighted avg     0.9209    0.9006    0.9090     78826\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "sgd_clf_with_fs = Pipeline(steps=[\n",
    "    (\"feature_selec\", SelectFromModel(\n",
    "        estimator=LogisticRegression(max_iter=5000, class_weight=\"balanced\", random_state=42),\n",
    "        threshold=\"median\"\n",
    "    )),\n",
    "    (\"classifier\", SGDClassifier(loss=\"log\", class_weight=\"balanced\", random_state=42))\n",
    "])\n",
    "\n",
    "evaluate_model(sgd_clf_with_fs, X_train_resampled, y_train_resampled, X_test, y_test, curves=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "#### **#3 : Fine tuning and model assessment**\n",
    "\n",
    "In this final section, we will perform grid search combined with cross validation to find the best set of hyper parameters for each of our three models, which are :\n",
    "- Random Forest\n",
    "- Gradient Boosting\n",
    "- Stochastic Gradient Descent\n",
    "\n",
    "We will then pick the overall best performing model for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **#3.1 : Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "'''rf_params = {\n",
    "    \"n_estimators\": [500, 1000, 2000],\n",
    "    \"criterion\" : [\"entropy\"],\n",
    "    \"class_weight\": [\"balanced\"]\n",
    "}\n",
    "\n",
    "rf_gscsv = GridSearchCV(\n",
    "    estimator=RandomForestClassifier(),\n",
    "    param_grid=rf_params,\n",
    "    scoring=\"f1_macro\",\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "rf_gscsv = rf_gscsv.fit(X_train_new_features, y_train)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Random Forest Best Model ====\n",
      "f1-score validation :  0.7494  with  {'class_weight': 'balanced', 'criterion': 'entropy', 'n_estimators': 2000}\n",
      "f1-score on test set :  0.7552\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      " Low income (<50k)     0.9536    0.9855    0.9693     72669\n",
      "High income (>50k)     0.7177    0.4343    0.5411      6157\n",
      "\n",
      "          accuracy                         0.9425     78826\n",
      "         macro avg     0.8356    0.7099    0.7552     78826\n",
      "      weighted avg     0.9352    0.9425    0.9359     78826\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"===== Random Forest Best Model ====\")\n",
    "\n",
    "print(\"f1-score validation : \", round(rf_gscsv.best_score_, 4), \" with \", rf_gscsv.best_params_)\n",
    "\n",
    "y_pred = rf_gscsv.best_estimator_.predict(X_test_new_features)\n",
    "score = f1_score(y_test, y_pred, average=\"macro\")\n",
    "print(\"f1-score on test set : \", round(score, 4))\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=[\"Low income (<50k)\", \"High income (>50k)\"], digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **#3.2 : Gradient Boosting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''gb_params = {\n",
    "    \"n_estimators\": [100, 500],\n",
    "    \"learning_rate\": [0.1],\n",
    "    \"max_depth\": [3, 5, 10],\n",
    "}\n",
    "\n",
    "gb_gscsv = GridSearchCV(\n",
    "    estimator=GradientBoostingClassifier(),\n",
    "    param_grid=gb_params,\n",
    "    scoring=\"f1_macro\",\n",
    "    verbose=1,\n",
    "    cv=3\n",
    ")\n",
    "\n",
    "gb_gscsv = gb_gscsv.fit(X_train_new_features, y_train)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Gradient Boosting Best Model ====\n",
      "f1-score validation :  0.7795  with  {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 500}\n",
      "f1-score on test set :  0.7638\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      " Low income (<50k)     0.9562    0.9821    0.9690     72669\n",
      "High income (>50k)     0.6893    0.4695    0.5586      6157\n",
      "\n",
      "          accuracy                         0.9420     78826\n",
      "         macro avg     0.8228    0.7258    0.7638     78826\n",
      "      weighted avg     0.9354    0.9420    0.9369     78826\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"===== Gradient Boosting Best Model ====\")\n",
    "\n",
    "print(\"f1-score validation : \", round(gb_gscsv.best_score_, 4), \" with \", gb_gscsv.best_params_)\n",
    "\n",
    "y_pred = gb_gscsv.best_estimator_.predict(X_test_new_features)\n",
    "score = f1_score(y_test, y_pred, average=\"macro\")\n",
    "print(\"f1-score on test set : \", round(score, 4))\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=[\"Low income (<50k)\", \"High income (>50k)\"], digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **#3.3 : SGD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''sgd_params = {\n",
    "    \"alpha\": [0.0001, 0.001, 0.01],\n",
    "    \"penalty\": [\"l1\", \"l2\"],\n",
    "    \"loss\": [\"log\"],\n",
    "    \"class_weight\": [\"balanced\"],\n",
    "    \"max_iter\": [2000]\n",
    "}\n",
    "\n",
    "sgd_gscsv = GridSearchCV(\n",
    "    estimator=SGDClassifier(),\n",
    "    param_grid=sgd_params,\n",
    "    scoring=\"f1_macro\",\n",
    "    verbose=1,\n",
    "    cv=3\n",
    ")\n",
    "\n",
    "sgd_gscsv = sgd_gscsv.fit(X_train_resampled, y_train_resampled)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== SGD Best Model ====\n",
      "f1-score validation :  0.925  with  {'alpha': 0.0001, 'class_weight': 'balanced', 'loss': 'log', 'max_iter': 2000, 'penalty': 'l2'}\n",
      "f1-score on test set :  0.7328\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      " Low income (<50k)     0.9688    0.9276    0.9478     72669\n",
      "High income (>50k)     0.4313    0.6477    0.5178      6157\n",
      "\n",
      "          accuracy                         0.9058     78826\n",
      "         macro avg     0.7001    0.7877    0.7328     78826\n",
      "      weighted avg     0.9268    0.9058    0.9142     78826\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"===== SGD Best Model ====\")\n",
    "\n",
    "print(\"f1-score validation : \", round(sgd_gscsv.best_score_, 4), \" with \", sgd_gscsv.best_params_)\n",
    "\n",
    "y_pred = sgd_gscsv.best_estimator_.predict(X_test)\n",
    "score = f1_score(y_test, y_pred, average=\"macro\")\n",
    "print(\"f1-score on test set : \", round(score, 4))\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=[\"Low income (<50k)\", \"High income (>50k)\"], digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **#3.4 : Best model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After tuning and testing, we can find the best performing model to be **Gradient Boosting**, a tree-based model, with an average f1-score of **76,38%**\n",
    "\n",
    "The score is pretty low, especially on the \"high income\" class (minority class). While we managed to improve precision on this class by quite a bit, recall is still low. \n",
    "\n",
    "Before concluding, let us take a final look at feature importance for our best fitted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>capital gains</th>\n",
       "      <td>0.237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dividends from stocks</th>\n",
       "      <td>0.128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weeks worked in year</th>\n",
       "      <td>0.098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sex_Female</th>\n",
       "      <td>0.095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>capital losses</th>\n",
       "      <td>0.050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>major occupation code_Executive admin and managerial</th>\n",
       "      <td>0.046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>0.046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>major occupation code_Professional specialty</th>\n",
       "      <td>0.039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>education_Prof school degree (MD DDS DVM LLB JD)</th>\n",
       "      <td>0.033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>education_Masters degree(MA MS MEng MEd MSW MBA)</th>\n",
       "      <td>0.030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>education_Bachelors degree(BA AB BS)</th>\n",
       "      <td>0.025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>education_Doctorate degree(PhD EdD)</th>\n",
       "      <td>0.024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>detailed occupation recode_2</th>\n",
       "      <td>0.022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb of employees in company estimate</th>\n",
       "      <td>0.022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tax filer stat_Joint both under 65</th>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>earnings estimate</th>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class of worker_Self-employed-incorporated</th>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>detailed occupation recode_4</th>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num persons worked for employer_6</th>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tax amount estimate</th>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    score\n",
       "capital gains                                       0.237\n",
       "dividends from stocks                               0.128\n",
       "weeks worked in year                                0.098\n",
       "sex_Female                                          0.095\n",
       "capital losses                                      0.050\n",
       "major occupation code_Executive admin and manag...  0.046\n",
       "age                                                 0.046\n",
       "major occupation code_Professional specialty        0.039\n",
       "education_Prof school degree (MD DDS DVM LLB JD)    0.033\n",
       "education_Masters degree(MA MS MEng MEd MSW MBA)    0.030\n",
       "education_Bachelors degree(BA AB BS)                0.025\n",
       "education_Doctorate degree(PhD EdD)                 0.024\n",
       "detailed occupation recode_2                        0.022\n",
       "nb of employees in company estimate                 0.022\n",
       "tax filer stat_Joint both under 65                  0.008\n",
       "earnings estimate                                   0.008\n",
       "class of worker_Self-employed-incorporated          0.007\n",
       "detailed occupation recode_4                        0.007\n",
       "num persons worked for employer_6                   0.007\n",
       "tax amount estimate                                 0.006"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_imp = pd.DataFrame(data=gb_gscsv.best_estimator_.feature_importances_, index=X_train_new_features.columns, columns=[\"score\"])\n",
    "feature_imp.sort_values(\"score\", ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "#### **Future improvements**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Collect **additional observations** of the minority class to resolve balancing issue.\n",
    "- Perform more **feature engineering** with domain expertise and gathering of extra data if necessary\n",
    "- Take a deeper dive into **feature importance**, for instance by computing Shapley values after modelling\n",
    "- Further investigate **dependencies in-between features** to avoid multicollinearity"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7b0d1bc37e4150564005d2f0757eca3230f3c8295ef7a49a1da484b71996b598"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
